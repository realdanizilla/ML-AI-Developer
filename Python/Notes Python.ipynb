{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python básico para AI\n",
    "\n",
    "VScode, \n",
    "\n",
    "Jamais faça deploy de notebook em produção. notebook é pra prototipação não para deploy\n",
    "\n",
    "Google colab\n",
    "\n",
    "Usar o codespaces dentro do github pois é ubuntu/linux\n",
    "\n",
    "para criar o ambiente virtual:\n",
    "- python -m venv venv\n",
    "  \n",
    "para ativar\n",
    "- source venv/bin/activate\n",
    "\n",
    "para instalar bibliotecas pode usar o poetry ou pip instal dentro do venv\n",
    "\n",
    "List comprehension\n",
    "- \"expressao com variavel\" for \"variavel\" in \"iterável\" if condição\n",
    "- ex: lista = [x**2 for x in range(11) if x % 2 ==0]\n",
    "\n",
    "dict comprehension\n",
    "```python\n",
    "nomes = [\"rapha\", \"maria\", \"jose\"]\n",
    "idades = [18, 24, 30]\n",
    "dicionario = {k:v for (k,v) in zip(nomes,idades) if k != 'rapha'}\n",
    "```\n",
    "\n",
    "## Generators\n",
    "\n",
    "ex: range\n",
    "\n",
    "```python\n",
    "def my_generator(max_num):\n",
    "    x = 0\n",
    "    while x < max_num:\n",
    "        yield x\n",
    "        x += 1\n",
    "gen = my_generator(3)\n",
    "print(next(gen))\n",
    "```\n",
    "nesse caso não usa memória\n",
    "\n",
    "## packages e módulos\n",
    "\n",
    "- um arquivo .py é um módulo\n",
    "- boa prática:\n",
    "  - if __name__ == '__main__':\n",
    "  - nao importar usando *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pandas\n",
    "\n",
    "### series\n",
    "serie2 = serie.apply(funcao) - jeito ruim\n",
    "\n",
    "serie2 = serie.apply(lambda x: x**2)\n",
    "\n",
    "### dataframes\n",
    "\n",
    "df = pd.DataFrame([serie1, serie2])\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"nome\": [\"rapha\", \"maria\", \"joao\"],\n",
    "    \"estado\": [\"sp\", \"sp\", \"rj\"],\n",
    "    \"renda\": [50_000, 400_000, 1_000_000]\n",
    "})\n",
    "\n",
    "filtrando:\n",
    "\n",
    "df[(df['Peso'] > 60) & (df['renda'] > 50000)][\"renda\"].mean()\n",
    "\n",
    "\n",
    "### carregando e salvando arquivo de dados\n",
    "\n",
    "pd.read_csv ou pd.read_excel ou pd.read_json\n",
    "\n",
    "df.to_csv(\"exemplo.csv\", index=False)\n",
    "\n",
    "df.to_excel(\"exemplo.xlsx\", index=False, sheet_name=\"dados\")\n",
    "\n",
    "df.to_json(\"exemplo.json\", index=False, orient=\"records\")\n",
    "\n",
    "\n",
    "### loc e iloc\n",
    "\n",
    "loc navega por indice de linha ou coluna\n",
    "\n",
    "loc aceita filtro\n",
    "\n",
    "iloc navega por posições\n",
    "\n",
    "iloc é mais usado\n",
    "\n",
    "usa indice: ou :indice ou indice:indice para fatiar\n",
    "\n",
    "### renomeando colunas\n",
    "\n",
    "temp = {\"Nome\":\"nome\", \"Idade\":\"idade\"}\n",
    "\n",
    "df = df.rename(columns=temp)\n",
    "\n",
    "evite mexer nos indices com set_index e coisas assim\n",
    "\n",
    "### lidando com nulos\n",
    "\n",
    ".info()\n",
    "\n",
    "isna().sum()\n",
    "\n",
    "df.fillna({\"nome\":\"desconhecido\", \"idade\":df[\"idade\"].mean(), \"cidade\":\"nao informada\"})\n",
    "\n",
    ".dropna()\n",
    "\n",
    "\n",
    "### apply e map\n",
    "\n",
    "apply pega uma função e aplica para tudo\n",
    "\n",
    "```python\n",
    "def aumentar_salario(salario):\n",
    "    return salario * 1,1\n",
    "\n",
    "df['salario_ajustado'] = df['salario'].apply(aumentar_salario)\n",
    "\n",
    "# ou usando funcao anonima\n",
    "\n",
    "df['salario_ajustado'] = df['salario'].apply(lambda salario: salario * 1,2)\n",
    "\n",
    "df['departmento'].value_counts()\n",
    "\n",
    "# map mapeia estrutura de uma coluna para outra, passando um dicionario de como corresponder\n",
    "\n",
    "temp = {\n",
    "    \"marketing\": \"diretoria financeira\",\n",
    "    \"vendas\": \"diretoria comercial\",\n",
    "}\n",
    "\n",
    "df[\"diretoria\"] = df[\"departamento\"].map(temp)\n",
    "\n",
    "```\n",
    "\n",
    "### groupby\n",
    "\n",
    "groupby(coluna ou lista de colunas)[coluna ou lista de colunas a trazer].operacao()\n",
    "\n",
    "groupby(coluna ou lista de colunas).agg({'salario_ajustado':['count', 'sum', 'mean','std, 'min','max']})\n",
    "\n",
    "### concat, merge, join\n",
    "\n",
    "concat é uniao simples (um em cima do outro ou um ao lado do outro)\n",
    "\n",
    "df_concat = pd.concat([df1,df2]) - usar axis=1 para coloar ao lado e nada ou axis=0 para colocar embaixo\n",
    "\n",
    "para juntar informacoes com base em uma coluna comum usar o merge\n",
    "\n",
    "df_merge = pd.merge(df1,df3,on=\"ID\") - usar how= para definir como dar o merge\n",
    "\n",
    "join é usado quando podemos usar o index do dataframe para unir as tabelas\n",
    "\n",
    "df_join = df1.join(df3, how='left')\n",
    "\n",
    "\n",
    "# EDA\n",
    "\n",
    ".describe()\n",
    "\n",
    ".nunique()\n",
    "\n",
    "df[\"departamento\"].value_counts()\n",
    "\n",
    "df[[\"idade\",\"salario\"]].corr()\n",
    "\n",
    "# datas\n",
    "\n",
    "df['coluna'] = pd.to_datetime(df['coluna'])\n",
    "\n",
    "dt.year, dt.month, dt.day, dt.month_name()\n",
    "\n",
    "test = pd.date_range(start='2024-01-01', end='2024-12-01', freq='M') ou freq=Q, d, h, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy\n",
    "\n",
    "np.array([elementos])\n",
    "\n",
    ".ndim\n",
    "\n",
    ".shape mostra quantos elementos em cada dimensão\n",
    "\n",
    "e[1,2,3] para acessar o quarto item na dimensao 3, terceiro na dimensão 2 e segundo na dimensão 1\n",
    "\n",
    "d += 1 soma 1 a todos os elementos da matriz d\n",
    "\n",
    "np.min, np.max, np.std, np.mean\n",
    "\n",
    "np.zeros(shape que queremos) ex: np.zeros(3,4) - cria matriz com zeros\n",
    "\n",
    "np.ones(shape que queremos) ex: np.ones(2,3) - cria matriz com uns\n",
    "\n",
    "np.full(shape, numero) ex: np.full(2,3, 5.65)\n",
    "\n",
    "np.arange(shape) ex: np.arange(10,20) cria uma matriz começando em 10 e terminando em 19\n",
    "\n",
    "np.linspace(inicial,final,qtde entre ini e fin) ex: np.linspace(0,1,5) cria uma matriz com 0, 0.25, 0.5, 0.75 e 1 (5 elementos)\n",
    "\n",
    "np.eye(tamanho da matriz identidade) ex: np.eye(3) cria uma matriz identidade (1s na diagonal e 0s no resto) no shape 3x3\n",
    "\n",
    "a + b soma arrays\n",
    "a - b subtrai arrays\n",
    "a * b multiplica arrays - não é multiplicação de matriz que precisa n linhas de 1 = n colunas da outra\n",
    "a / b divide arrays\n",
    "\n",
    "np.sin(array) retorna o seno\n",
    "\n",
    "np.exp(array) exponencia o array\n",
    "\n",
    "## algebra linear\n",
    "\n",
    "np.dot(a,b) multiplica matrizes que atendam a condição ou usar np.matmul(a,b)\n",
    "\n",
    "np.linalg.det(matriz) retorna o determinante da matriz\n",
    "\n",
    "np.linalg.inv(matriz) retorna a matriz invertida\n",
    "\n",
    "se multiplica uma matriz pela sua invertida, temos uma matriz identidade\n",
    "\n",
    "para calcular autovalor e autovetor usar:\n",
    "\n",
    "autovalor, autovetor = np.linalg.eig(matriz)\n",
    "\n",
    "## slicing arrays\n",
    "\n",
    "ex uma matriz a = np.array([[1,2,3] [10,20,30] [100,200,300]])\n",
    "\n",
    "para acessar a 1a linha usar a[0,:]\n",
    "\n",
    "para acessar a 1a coluna usar a[:,1]\n",
    "\n",
    "para acessar todas as linhas, colunas 2 e 3 usar a[:,1:3]\n",
    "\n",
    "filtros:\n",
    "\n",
    "a(a>15) - retorna valores maiores que 15 - dessa forma perdemos a estrutura matricial\n",
    "\n",
    "## reshape de arrays\n",
    "\n",
    "matriz.T transpoe uma matriz (inverte linha e coluna)\n",
    "\n",
    "matriz.ravel() transforma a matriz em um vetor unidimensional com todos os elementos\n",
    "\n",
    "reshape muda a matriz mas precisa ter o mesmo número de posições da matriz original, ex matriz original 4x3 pode virar uma 6x2 mas nao uma 6x1. Multiplica os dois shapes e tem que dar o mesmo resultado (12 neste caso)\n",
    "\n",
    "reshape não cria cópia, ele cria uma referencia, entao se mudar o reshape de uma matriz, vai alterar a matriz original além da reshapada pois não alocou outro espaço para a memória. usar para reduzir uso de memória. para criar uma cópia usamos o resize\n",
    "\n",
    "np.resize(matriz, (linhas,colunas)) - neste caso podemos alterar pois nao muda a matriz original pois aloca outro espaço na memória\n",
    "\n",
    "## broadcasting\n",
    "\n",
    "usar com cautela pois é meio confuso\n",
    "\n",
    "expande dimensoes de matrizes para que seja possivel realizar operacoes como soma com outra matriz, seja criando linhas ou colunas em uma matriz para que ela possa ser operada\n",
    "\n",
    "podemos também multiplicar um array por um numero (produto escalar)\n",
    "\n",
    "b * 42 irá multiplicar cada elemento de b por 42\n",
    "\n",
    "normalização\n",
    "1. calcular a média de cada coluna da matriz com np.mean(matriz, axis=0)\n",
    "2. calcular desvio de cada coluna da matriz com np.std(matriz,axis=0)\n",
    "3. normalizar com data_norm = (matriz - media) / std\n",
    "\n",
    "## deep copy\n",
    "\n",
    "usar um .copy após selecionar a fatia que se quer de uma matriz. Assim deixa de ser uma referencia e passa a ter um espaço na memoria \n",
    "\n",
    "cuidado ao fazer slicing, verifica se vai alterar alguma coisa na matriz e como essa alteração pode afetar a matriz original\n",
    "\n",
    "## carregar e salvar\n",
    "\n",
    "numpy é rápido pois usa programacao c\n",
    "\n",
    "np.savetxt('nome.txt', variavel, fmt='%d') - fmt é o formato de número (int , decimal e quantas casas, ver documentação)\n",
    "\n",
    "np.loadtxt('nome.txt', dtype=int) - ou float, string, etc, depende do que vc precisa\n",
    "\n",
    "np.savetxt('nome.csv', variavel, delimiter=',', fmt='%d')\n",
    "\n",
    "para gerar um arquivo binario\n",
    "np.save('nome.npy',variavel)\n",
    "\n",
    "para carregar\n",
    "np.load('nome.npy')\n",
    "\n",
    "## vetorização\n",
    "\n",
    "usar calculo vetorizado é mais rapido que usar for e coisas assim\n",
    "\n",
    "```python\n",
    "def quadrado(data):\n",
    "    result = np.empty(len(data))\n",
    "    for i in range(len(data)):\n",
    "        result[i] = data[i] ** 2\n",
    "    return result\n",
    "\n",
    "def quadrado_vec(data):\n",
    "    return data ** 2\n",
    "\n",
    "```\n",
    "a segunda função é 130 vezes mais rápida que a primeira\n",
    "\n",
    "## memmap\n",
    "\n",
    "usar o HD alem da RAM para lidar com muitos dados. Cria arquivo no hd e pegamos parte dele para ir fazendo o processamento\n",
    "\n",
    "np.memmap(file_name,dtype=data_type,mode='w+',shape=shape) - w é write\n",
    "\n",
    ".fill(numero) preenche um array com um número\n",
    "\n",
    "é uma shallow copy (referencia) se mudar em uma matriz referencia, vai mudar a original\n",
    "\n",
    "```python\n",
    "file_name = 'data.dat'\n",
    "data_type = 'np.float64'\n",
    "shape = (10_000,10_000)\n",
    "\n",
    "big_array = np.memmap(file_name,dtype=data_type,mode='w+',shape=shape)\n",
    "big_array.fill(5.0)\n",
    "sub_big_array=big_array[:100,:100]\n",
    "sub_big_array += 10\n",
    "del_sub_big_array # sincroniza os dados atuais com o arquivo\n",
    "del_big_array # sincroniza os dados atuais com o arquivo\n",
    "\n",
    "big_array2 = np.memmap(file_name,dtype=data_type,mode='r',shape=shape)\n",
    "big_array2[:99,:100] # tá cheio de numero 15\n",
    "big_array2[:100,:100] # ta tudo numero 5, porque multiplcamos por partes\n",
    "\n",
    "```\n",
    "\n",
    "## funções de probabilidade e gráficos\n",
    "\n",
    "para gerar um histograma do numpy\n",
    "\n",
    "hist,bins = np.histogram(normal,bins=20)\n",
    "\n",
    "para plotar com matplotlib\n",
    "\n",
    "plt.bar(bins[:-1],hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit Learn\n",
    "\n",
    "usada para DS e ML\n",
    "\n",
    "métodos fit, transform e predict\n",
    "\n",
    "fit treina o modelo\n",
    "\n",
    "transform muda os dados\n",
    "\n",
    "predict prevê\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris, load_digits\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "iris = load_iris()\n",
    "X_iris, y_iris = iris.data, iris.target\n",
    "scale = StandardScaler() # escala os dados para que tenham média 0 e desvio 1\n",
    "scale.fit(X_iris)\n",
    "X_iris_std = scale.transform(X_iris)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_iris, y_iris)\n",
    "X_iris_std[0] # é o primeiro array de observações\n",
    "y_iris[0] # é a classificação da primeira iris\n",
    "model.predict(X_iris_std[0].reshape(1,-1)) # com base na primeira observação, preveja o resultado (0 no caso). O reshape é usado pq precisamos de um 2d e X_iris_std[0] tem shape de 1d\n",
    "```\n",
    "\n",
    "## normalizando dados\n",
    "```python\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "wine_data = load_wine()\n",
    "X = wine_data.data\n",
    "y = wine_data.target\n",
    "df = pd.DataFrame(X, columns=wine_data.feature_names)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n",
    "\n",
    "# para normalizar na mão usando standard scaler\n",
    "(X_train[:,1] - X_train[:,1].mean()) / X_train[:,1].std()\n",
    "\n",
    "# normalizar usando método standardscaler\n",
    "scale = StandardScaler()\n",
    "\n",
    "X_train_std = scale.fit_transform(X_train)\n",
    "\n",
    "# normalizar na mao min max - todos os valores entre zero e um\n",
    "(X_train[:,1] - X_train[:,1].min()) / (X_train[:,1].max() - X_train[:,1].min())\n",
    "\n",
    "scale2 = MinMaxScaler()\n",
    "\n",
    "X_train_min_max = scale2.fit_transform(X_train)\n",
    "\n",
    "```\n",
    "\n",
    "## lidando com variáveis categórias\n",
    "\n",
    "one hot encoder e label encoder\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"cor\": [\"vermelho\", \"azul\", \"Verde\", \"azul\", \"Verde\", \"vermelho\"],\n",
    "    \"tamanho\": [\"P\", \"M\", \"G\",\"G\", \"P\", \"M\"],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df\n",
    "label_encoder = LabelEncoder() # usar mais quando as variáveis categóricas tem noção de grandeza entre elas, como tamanho da roupa\n",
    "df[\"tamanho_trans\"] = label_encoder.fit_transform(df[\"tamanho\"])\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False) # nao carrega os zeros na memória\n",
    "df_cor = pd.DataFrame(ohe.fit_transform(df[[\"cor\"]]),columns=ohe.get_feature_names_out()) # criando dataframe a partir do dataframe e não da serie\n",
    "\n",
    "```\n",
    "\n",
    "## imputação de dados\n",
    "\n",
    "```python\n",
    "from sklearn.imput import SimpleImputer # simples, pode ficar mais sofisticado\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"idade\":[25, np.nan, 35, 60, 45, np.nan, 30, 56, 45],\n",
    "    \"salario\":[50000, 60000, 75000, np.nan, 120000, 95000, np.nan, 87454, 45000],\n",
    "    \"depto\": [\"rh\", \"financas\", \"financas\", \"TI\", np.nan, \"rh\", \"financas\", np.nan, \"financas\"]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df\n",
    "\n",
    "impute_mean = SimpleImputer(strategy='mean') # media para idade em mercado de trabalho 18-60 anos faz sentido\n",
    "df[[\"idade\"]] = impute_mean.fit_transform(df[[\"idade\"]])\n",
    "\n",
    "inpute_median = SimpleImputer(strategy='median')\n",
    "df[[\"salario\"]] = impute_median.fit_transform(df[[\"salario\"]]) # pq salario é cauda longa, poucos ganham muito e muitos ganham pouco\n",
    "\n",
    "inpute_most_frequent = SimpleImputer(strategy='most_frequent') # é categórica, logo não tem média ou mediana entao vamos com a moda\n",
    "df[[\"depto\"]] = impute_most_frequent.fit_transform(df[[\"depto\"]])\n",
    "```\n",
    "\n",
    "## binarização (simplificação do problema)\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import Binarizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"pontuacao\": [10, 21, 30, 40, 50, 60, 70],\n",
    "    \"idade\": [18, 22, 25, 21, 19, 40, 45]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df\n",
    "\n",
    "pontuacao_binaria = Binarizer(threshold=30) # threshold não incluído então <30 é 0 e >= 30 é 1\n",
    "idade_binaria = Binarizer(threshold=20)\n",
    "\n",
    "df[['pontuacao_bin']] = pontuacao_binaria.fit_transform(df[['pontuacao']])\n",
    "df[['idade_bin']] = idade_binaria.fit_transform(df[['idade']])\n",
    "```\n",
    "\n",
    "## validação cruzada\n",
    "\n",
    "resolve o dilema vies x variancia\n",
    "- vies ocorre quando o modelo nao se ajusta muito bem aos dados de treino e erra a previsão.  \n",
    "- variancia - ajusta bem aos dados de treino mas nao generaliza entao quando tem dados novos, erra a previsão\n",
    "- queremos encontrar o equilíbrio, entao usamos validação cruzada\n",
    "- a ideia é dividir os dados em caixinhas (k-folds) e usar 1 caixa para teste e as demais para treinar o modelo, daí ao final tiramos a média e desvio do resultado e temos uma medida mais robusta do modelo\n",
    "- underfitting significa viés alto\n",
    "- overfitting significa variancia alta\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import load_breast_cancer\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, KFold, RepeatedKFold, LeaveOneOut\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "breast_cancer = load_breast_cancer()\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "\n",
    "model = SVC(kernel=\"linear\")\n",
    "kf = KFold(n_splits=5, shuffle=True, randon_state=42)\n",
    "rkf = RepeatedKFold(n_splits=5, n_repeats=2, randon_state=42)\n",
    "\n",
    "score = cross_validate(model, X, y, cv=kf, scoring='accuracy') # traz tb o tempo que levou o treinamento\n",
    "\n",
    "score2 = cross_Val_score(model, X, y, cv=kf, scoring='accuracy') # traz somente o score\n",
    "\n",
    "```\n",
    "O leave one out funciona para poucos dados pois vai deixar um dado de fora e rodar com todos os outros, entao para muitos dados nao faz sentido\n",
    "\n",
    "- o RepeatedKFold é usado quando queremos repetir a validação cruzada varias vezes\n",
    "- validação cruzada tem limite, especialmente quando estamos falando de bigdata pois pode demorar para rodar o modelo multiplas vezes\n",
    "\n",
    "\n",
    "## Métricas de desempenho\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_breast_cancer, fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "breast_cancer = load_breast_cancer()\n",
    "X_breast_cancer = breast_cancer.data\n",
    "y_breast_cancer = breast_cancer.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_breast_cancer,y_breast_cancer, test_size=0.2, random_state=42)\n",
    "\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "precision = precision_score(y_test,y_pred)\n",
    "recall =  recall_score(y_test,y_pred)\n",
    "print(f\"accuracy: {accuracy}\")\n",
    "print(f\"precision: {precision}\")\n",
    "print(f\"recall: {recall}\")\n",
    "\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "\n",
    "plt.figure(figsize(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Matrix de confusão\")\n",
    "plt.xlabel=(\"valores preditos\")\n",
    "plt.ylabel=(\"valores verdadeiros\")\n",
    "plt.show()\n",
    "\n",
    "california = fetch_california_housing()\n",
    "X_california, y_california = california.data, california.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_california,y_california, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestRegressor(n_stimators=100)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"mse: {mse}\")\n",
    "print(f\"r2: {r2}\")\n",
    "```\n",
    "\n",
    "## Automatizando fluxos ML pipeline\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "pipeline = Pipeline([ # lista de tuplas\n",
    "    ('standard', StandardScaler()),\n",
    "    ('PCA', PCA(n_components=2)),\n",
    "    ('l_regression', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "```\n",
    "\n",
    "\n",
    "## Otimizando modelos\n",
    "\n",
    "gridsearchCV e randomsearch\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# usando gridsearch\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, scoring='accuracy', cv=5, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_\n",
    "grid_sdearch.best_score_\n",
    "\n",
    "# agora usando randomsearch - a dica aqui é ir investigando áreas de range e ver o que performa melhor\n",
    "param_distributions = {\n",
    "    'n_estimators': np.arange(10,510),\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [np.arange(2,100)]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_distributions, n_iter=10, scoring='accuracy', cv=5, verbose=1)\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "random_search.best_params_\n",
    "random_search.best_score_\n",
    "\n",
    "```\n",
    "\n",
    "## salvando e carregando modelos\n",
    "\n",
    "para guardar, trazer ou mandar para produção\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from joblib import dump, load # otimizado para matrizes gigante e big data\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "# salvando o modelo\n",
    "dump(model, 'iris_rf.joblib')\n",
    "\n",
    "# carregando o modelo\n",
    "load_model = load('iris_rf.joblib')\n",
    "y_pred = load_model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programação orientada a objetos\n",
    "\n",
    "## classes, objetos, métodos e classes\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "class My_LinearRegression: # classe é uma fábrica de coisas, um template. atributo cria variáveis e método cria funções\n",
    "    def __init__(self): # sempre necessário para classes pois define parâmetros iniciais\n",
    "        self.coeficients = None # coeficients é um atributo da classe\n",
    "        self.intercept = None # intercept é um atributo da classe\n",
    "\n",
    "    def fit(self,X,y): # método da classe\n",
    "        Xb = np.c_[np.ones((X.shape[0],1)), X] #concatenar valores de X com 1 em duas colunas, uma com o X e outra com 1s. Ex: 1,1 ;  1,2 ; 1,3 ; 1,4, etc\n",
    "        theta = np.linalg.inv(Xb.T.dot(Xb)).dot(Xb.T).dot(y)   # (X'X)-1Xy -> multiplicação de matrizes\n",
    "        self.intercept = theta[0] # o primeiro elemento é o intercepto\n",
    "        self.coeficients = theta[1:] # os demais são os coeficients\n",
    "\n",
    "    def predict(self, X): # método da classe\n",
    "        Xb = np.c_[np.ones((X.shape[0],1)), X]\n",
    "        return Xb.dot(np.r_[self.intercept, self.coeficients]) # r_ é juntar intercepto com os coeficients, daí multiplicar por Xb\n",
    "\n",
    "if __name__ == __main__:\n",
    "    X_train = np.array ([[1], [2], [3], [4], [5]])\n",
    "    y_train = np.array ([2, 4, 6, 8, 10])\n",
    "    \n",
    "    model = My_LinearRegression()\n",
    "    \n",
    "    print(model.coeficients)\n",
    "    print(model.intercept)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    print(model.coeficients)\n",
    "    print(model.intercept)\n",
    "    \n",
    "    X_test = np.array([[6], [7], [8]])\n",
    "    \n",
    "    predictions = model.predict(X_test)\n",
    "    print(\"previsões:\", predictions)\n",
    "\n",
    "    X_train = np.array ([[1], [2], [3], [4], [5]])\n",
    "    y_train = np.array ([4, 7, 10, 13, 16])\n",
    "    \n",
    "    model2 = My_LinearRegression()\n",
    "    \n",
    "    print(model2.coeficients)\n",
    "    print(model2.intercept)\n",
    "\n",
    "    model2.fit(X_train, y_train)\n",
    "    print(model2.coeficients)\n",
    "    print(model2.intercept)\n",
    "\n",
    "    X_test = np.array([[6], [7], [8]])\n",
    "    \n",
    "    predictions = model2.predict(X_test)\n",
    "    print(\"previsões:\", predictions)\n",
    "\n",
    "    X_test = np.array([[9], [10], [11]])\n",
    "    \n",
    "    predictions = model.predict(X_test)\n",
    "    print(\"previsões:\", predictions)\n",
    "\n",
    "    predictions = model2.predict(X_test)\n",
    "    print(\"previsões:\", predictions)\n",
    "\n",
    "```\n",
    "## 4 pilares - encapsulamento, herança, polimorfismo, abstração\n",
    "\n",
    "### classe abstrata\n",
    "\n",
    "- classe onde todas outras classes derivam dela, seguem o seu modelo. Seria a classe base da qual outras sao criadas\n",
    "- seria como um pré-projeto, para que projetos possam ser criados a partir dela\n",
    "\n",
    "### herança\n",
    "\n",
    "- quando definimos que uma classe filha deve herdar coisas da classe mãe/pai\n",
    "\n",
    "### encapsulamento\n",
    "\n",
    "- colocar uma coisa dentro da outra, cria uma proteção, invólucro\n",
    "\n",
    "### polimorfismo\n",
    "\n",
    "- usar um mesmo objeto em várias camadas do código, nesse examplo usamos o evaluate para 2 modelos diferentes\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class BaseModel(ABC): #ABC é usado quando queremos passar herança para classes filhas. Também indica que é uma classe abstrata que nunca será instanciada, somente as filhas serão\n",
    "    def __init__(self): # agora esse init pode vir para a classe abstrata ao invés de estar na classe filha, porém isso não é boa prática pois nem todas as classes filhas terão os componentes abaixo (coeficients e intercept),avaliar caso a caso\n",
    "        self.coeficients = None \n",
    "        self.intercept = None \n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self, X, y): # aqui dizemos que toda classe derivada dessa deve ter o método fit, se nao tiver vai dar erro ao instanciar a classe filha\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X): # aqui dizemos que toda classe derivada dessa deve ter o método predict, se nao tiver vai dar erro ao instanciar a classe filha\n",
    "        pass\n",
    "\n",
    "class My_LinearRegression(BaseModel): # identificar Basemodel indica que essa classe filha herda coisas da classe mãe/pai\n",
    "    def __init__(self): # supondo que esses atributos são só da classe My_LinearRegression\n",
    "        self._coeficients = None # o underline aqui ou em um método indica que ele é privado, ou seja, só deve ser utilizado dentro da classe, não alterar fora da classe usando por exemplo model._intercept = -10000. É uma convenção e não regra pq se vc tentar alterar vc vai conseguir\n",
    "        self._intercept = None \n",
    "\n",
    "    def fit(self,X,y): # método da classe\n",
    "        Xb = np.c_[np.ones((X.shape[0],1)), X] #concatenar valores de X com 1 em duas colunas, uma com o X e outra com 1s. Ex: 1,1 ;  1,2 ; 1,3 ; 1,4, etc\n",
    "        theta = np.linalg.inv(Xb.T.dot(Xb)).dot(Xb.T).dot(y)   # (X'X)-1Xy -> multiplicação de matrizes\n",
    "        self._intercept = theta[0] # o primeiro elemento é o intercepto\n",
    "        self._coeficients = theta[1:] # os demais são os coeficients\n",
    "\n",
    "    def predict(self, X): # método da classe\n",
    "        Xb = np.c_[np.ones((X.shape[0],1)), X]\n",
    "        return Xb.dot(np.r_[self._intercept, self._coeficients]) # r_ é juntar intercepto com os coeficients, daí multiplicar por Xb\n",
    "\n",
    "    def mse(self, y_true, y_pred):\n",
    "        return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "    @property\n",
    "    def coeficients(self):\n",
    "        return self._coeficients\n",
    "\n",
    "    @property\n",
    "    def intercept(self):\n",
    "        return self._intercept\n",
    "\n",
    "class My_LinearRegression(BaseModel): # identificar Basemodel indica que essa classe filha herda coisas da classe mãe/pai\n",
    "    def __init__(self): # supondo que esses atributos são só da classe My_LinearRegression\n",
    "        self._coeficients = None # o underline aqui ou em um método indica que ele é privado, ou seja, só deve ser utilizado dentro da classe, não alterar fora da classe usando por exemplo model._intercept = -10000. É uma convenção e não regra pq se vc tentar alterar vc vai conseguir\n",
    "        self._intercept = None \n",
    "\n",
    "    def fit(self,X,y): # método da classe\n",
    "        Xb = np.c_[np.ones((X.shape[0],1)), X] #concatenar valores de X com 1 em duas colunas, uma com o X e outra com 1s. Ex: 1,1 ;  1,2 ; 1,3 ; 1,4, etc\n",
    "        theta = np.linalg.inv(Xb.T.dot(Xb)).dot(Xb.T).dot(y)   # (X'X)-1Xy -> multiplicação de matrizes\n",
    "        self._intercept = theta[0] # o primeiro elemento é o intercepto\n",
    "        self._coeficients = theta[1:] # os demais são os coeficients\n",
    "\n",
    "    def predict(self, X): # método da classe\n",
    "        Xb = np.c_[np.ones((X.shape[0],1)), X]\n",
    "        return Xb.dot(np.r_[self._intercept, self._coeficients]) # r_ é juntar intercepto com os coeficients, daí multiplicar por Xb\n",
    "\n",
    "    def mse(self, y_true, y_pred):\n",
    "        return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "    @property\n",
    "    def coeficients(self):\n",
    "        return self._coeficients\n",
    "\n",
    "    @property\n",
    "    def intercept(self):\n",
    "        return self._intercept\n",
    "\n",
    "\n",
    "class My_LinearRegression_v2(BaseModel): \n",
    "    def __init__(self): \n",
    "        self._coeficients = None \n",
    "        self._intercept = None \n",
    "\n",
    "    def fit(self,X,y):\n",
    "        self._intercept = 0\n",
    "        self._coeficients = np.mean(y)\n",
    "\n",
    "    def predict(self, X): \n",
    "        return np.array(len(X) * [self._coeficients])\n",
    "\n",
    "    def mse(self, y_true, y_pred):\n",
    "        return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "    @property # property é a maneira segura de acessar esse atributo da classe\n",
    "    def coeficients(self):\n",
    "        return self._coeficients\n",
    "\n",
    "    @property\n",
    "    def intercept(self):\n",
    "        return self._intercept\n",
    "\n",
    "def evaluate(model: BaseModel, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = model.mse(y_test, y_pred)\n",
    "    print(f\"o MSE é: {mse}\")\n",
    "    print(\"coeficients:\", model.coeficients)\n",
    "    print(\"intercept:\", model.intercept)\n",
    "\n",
    "if __name__ == __main__:\n",
    "    X_train = np.array ([[1], [2], [3], [4], [5]])\n",
    "    y_train = np.array ([2, 4, 6, 8, 10])\n",
    "    X_test = np.array([[6], [7], [8]])\n",
    "    y_test = np.array([12, 14, 16, 18, 20])   \n",
    "    \n",
    "    model = My_LinearRegression()\n",
    "    model_v2 = My_LinearRegression_v2()\n",
    "\n",
    "    evaluate(model, X_train, y_train, X_test, y_test)\n",
    "    evaluate(model_v2, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # agora usando a v2\n",
    "    model = My_LinearRegression_v2()\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    X_test = np.array([[6], [7], [8]])\n",
    "    \n",
    "    predictions = model.predict(X_test)\n",
    "    print(\"previsões:\", predictions)\n",
    "    print(\"coeficients:\", model.coeficients)\n",
    "    print(\"intercept:\", model.intercept)\n",
    "```\n",
    "\n",
    "## delete\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class BaseModel(ABC): #ABC é usado quando queremos passar herança para classes filhas. Também indica que é uma classe abstrata que nunca será instanciada, somente as filhas serão\n",
    "    def __init__(self): # agora esse init pode vir para a classe abstrata ao invés de estar na classe filha, porém isso não é boa prática pois nem todas as classes filhas terão os componentes abaixo (coeficients e intercept),avaliar caso a caso\n",
    "        self.coeficients = None \n",
    "        self.intercept = None \n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self, X, y): # aqui dizemos que toda classe derivada dessa deve ter o método fit, se nao tiver vai dar erro ao instanciar a classe filha\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X): # aqui dizemos que toda classe derivada dessa deve ter o método predict, se nao tiver vai dar erro ao instanciar a classe filha\n",
    "        pass\n",
    "\n",
    "    def __del__(self): # executado um momento antes de excluir da memória. Usado para limpar memória, ter certeza q foi deletado, não muito usado em python, mais em Jva e C++ (destructor)\n",
    "        print(\"estou sendo deletado\")\n",
    "\n",
    "class My_LinearRegression(BaseModel): # identificar Basemodel indica que essa classe filha herda coisas da classe mãe/pai\n",
    "    def __init__(self): # supondo que esses atributos são só da classe My_LinearRegression\n",
    "        self._coeficients = None # o underline aqui ou em um método indica que ele é privado, ou seja, só deve ser utilizado dentro da classe, não alterar fora da classe usando por exemplo model._intercept = -10000. É uma convenção e não regra pq se vc tentar alterar vc vai conseguir\n",
    "        self._intercept = None \n",
    "\n",
    "    def fit(self,X,y): # método da classe\n",
    "        Xb = np.c_[np.ones((X.shape[0],1)), X] #concatenar valores de X com 1 em duas colunas, uma com o X e outra com 1s. Ex: 1,1 ;  1,2 ; 1,3 ; 1,4, etc\n",
    "        theta = np.linalg.inv(Xb.T.dot(Xb)).dot(Xb.T).dot(y)   # (X'X)-1Xy -> multiplicação de matrizes\n",
    "        self._intercept = theta[0] # o primeiro elemento é o intercepto\n",
    "        self._coeficients = theta[1:] # os demais são os coeficients\n",
    "\n",
    "    def predict(self, X): # método da classe\n",
    "        Xb = np.c_[np.ones((X.shape[0],1)), X]\n",
    "        return Xb.dot(np.r_[self._intercept, self._coeficients]) # r_ é juntar intercepto com os coeficients, daí multiplicar por Xb\n",
    "\n",
    "    def mse(self, y_true, y_pred):\n",
    "        return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "    @property\n",
    "    def coeficients(self):\n",
    "        return self._coeficients\n",
    "\n",
    "    @property\n",
    "    def intercept(self):\n",
    "        return self._intercept\n",
    "    \n",
    "    def __del__(self):\n",
    "        print(\"estou sendo deletado\")\n",
    "\n",
    "class My_LinearRegression_v2(BaseModel): \n",
    "    def __init__(self): \n",
    "        self._coeficients = None \n",
    "        self._intercept = None \n",
    "\n",
    "    def fit(self,X,y):\n",
    "        self._intercept = 0\n",
    "        self._coeficients = np.mean(y)\n",
    "\n",
    "    def predict(self, X): \n",
    "        return np.array(len(X) * [self._coeficients])\n",
    "\n",
    "    def mse(self, y_true, y_pred):\n",
    "        return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "    @property # property é a maneira segura de acessar esse atributo da classe\n",
    "    def coeficients(self):\n",
    "        return self._coeficients\n",
    "\n",
    "    @property\n",
    "    def intercept(self):\n",
    "        return self._intercept\n",
    "\n",
    "    def __del__(self):\n",
    "        print(\"estou sendo deletado\")\n",
    "\n",
    "if __name__ == __main__:\n",
    "    X_train = np.array ([[1], [2], [3], [4], [5]])\n",
    "    y_train = np.array ([2, 4, 6, 8, 10])\n",
    "    X_test = np.array([[6], [7], [8]])\n",
    "    y_test = np.array([12, 14, 16, 18, 20])   \n",
    "    \n",
    "    model = My_LinearRegression()\n",
    "    model_v2 = My_LinearRegression_v2()\n",
    "\n",
    "    evaluate(model, X_train, y_train, X_test, y_test)\n",
    "    del model\n",
    "    evaluate(model_v2, X_train, y_train, X_test, y_test)\n",
    "    del model_v2\n",
    "\n",
    "```\n",
    "\n",
    "## Métodos\n",
    "\n",
    "de instância\n",
    "\n",
    "- funções no nível do objeto\n",
    "\n",
    "de classe\n",
    "\n",
    "- acessa parametros da classe, instancia objetos com uma determinada configuração\n",
    "\n",
    "estático\n",
    "\n",
    "- função que não tem a ver com classe ou objeto, colocado normalmente dentro da classe\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class DataPreProcessor:\n",
    "    def __init__(self):\n",
    "        self.data = data\n",
    "\n",
    "    def normalize(self): # esse é um método de instância pois está associado a instância dessa classe, preprocessor e preprocessor2. Só consigo fazer as coisas dentro do objeto\n",
    "        self.data = (self.data - np.min(self.data, axis=0)) / (np.max(self.data, axis=0) - np.min(self.data, axis=0))\n",
    "    \n",
    "    @classmethod\n",
    "    def from_csv(cls,file_path): # nao tem self porque é da classe. Nos permite instanciar a classe já com algum parâmetro default\n",
    "        data = pd.read_csv(file_path).values\n",
    "        return cls(data)\n",
    "\n",
    "    @staticmethod \n",
    "    def validate(data): # pode ser usado tanto dentro da classe como fora\n",
    "        if np.any(pd.isnull(data)):\n",
    "            raise ValueError(\"os dados não podem conter valores null\")\n",
    "        print(\"esses dados são válidos\")\n",
    "\n",
    "\n",
    "data = np.random.rand(100,5) * 100\n",
    "# data[0,0] = np.nan  --> aqui estamos gerando um nan para fazer o método validate acusar o erro de dado null\n",
    "data2 = np.random.rand(1002,5) * 100\n",
    "\n",
    "print(data.shape())\n",
    "print(data2.shape())\n",
    "\n",
    "preprocessor = DataPreProcessor(data) # poderia instanciar um preprocessor2 = DataPreProcessor(data2) sem prejuízo do anterior\n",
    "print(preprocessor.data[:5])\n",
    "preprocessor.normalize()\n",
    "print(preprocessor.data[:5])\n",
    "\n",
    "\n",
    "\n",
    "# aqui estamos já carregando os dados na instancia preprocessorv2 usando file_path e nao data\n",
    "\n",
    "file_path = 'data.csv'\n",
    "pd.DataFrame(data).to_csv(file_path)\n",
    "\n",
    "# aqui estamos carregando os dados fora da classe\n",
    "data_v1 = pd.read_csv(file_path)\n",
    "preprocessor = DataPreProcessor.from_csv(data_v1)\n",
    "preprocessor.validate(data_v1)\n",
    "preprocessor.normalize()\n",
    "print(preprocessor.data[:5])\n",
    "\n",
    "\n",
    "# aqui estamos carregando os dados por dentro da classe\n",
    "preprocessorv2 = DataPreProcessor.from_csv(file_path) # objeto instanciado a partir de um class method\n",
    "preprocessorv2.normalize()\n",
    "print(preprocessorv2.data[:5])\n",
    "preprocessorv2.validate(data_v1)\n",
    "\n",
    "data2 = np.random.rand(100,5) * 100\n",
    "data2[0,0] = np.nan\n",
    "preprocessorv2.validate(data2)\n",
    "```\n",
    "\n",
    "## composição e agregação\n",
    "\n",
    "composição é quando uma coisa está dentro da outra - ex órgãos como pulmão, cérebro e coração fazem parte da composição do ser humano. O ser humano tem esses órgãos e esses órgãos não existem fora de um ser humano\n",
    "\n",
    "agregação é quando há uma relação mas ela não é tão forte, ex: escola e aluno, o aluno faz parte da escola mas a escola existe sem o aluno e o aluno existe sem a escola\n",
    "\n",
    "relações de composição e agregação são mais sutis e mais performáticas do que a relação de herança para nível de acoplamento \n",
    "\n",
    "Quanto mais acoplada, mais risco de ter impacto com uma mudança pequena. Saber quando usar acoplação em função da possibilidade de mudanças futuras nas classes e o impacto que isso gera\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class DataPreProcessor: # essa classe só existe para ser usada com a classe machinelearningmodel. Para essa classe existir ela precisa do modelo. Ela não tem outra utilidade caso nao seja usada para a classe machinelearningmodel - é uma composição com machinelearningmodel\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        return self.scaler.fit_transform(X)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.scaler.transform(X)\n",
    "\n",
    "class MachineLearningModel:\n",
    "    def __init__(self,preprocessor=None):\n",
    "        self.model = LogisticRegression()\n",
    "        self.preprocessor = preprocessor\n",
    "\n",
    "    def train(self, X, y):\n",
    "        if self.preprocessor:\n",
    "            X = self.preprocessor.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.preprocessor:\n",
    "            X = self.preprocessor.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "class ModelEvaluator: # essa classe pode existir independente da classe machinelearningmodel. Esta classe pode existir independente da machinelearningmodel e machinelearningmodel pode existir independente do evaluator - é uma agregação com machinelearningmodel\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        predictions = self.model.predict(X)\n",
    "        accuracy = np.mean(predictions == y)\n",
    "        print(F\"acuracia: {accuracy:.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X_train = np.array([[1,2],[2,3],[3,4],[4,5],[5,6]])\n",
    "    y_train = np.array([0,0,1,1,1])\n",
    "    \n",
    "    X_test = np.array([[1.5,2.5], [2.5,3,5], [3.5, 4.5]])\n",
    "    y_test = np.array([0,0,1])\n",
    "\n",
    "    preprocessor = DataPreProcessor()\n",
    "    model = MachineLearningModel(preprocessor=preprocessor)\n",
    "    \n",
    "    model.train(X_train,y_train)\n",
    "\n",
    "    evaluator = ModelEvaluator(model=model)\n",
    "    \n",
    "    evaluator.evaluate(X_test, y_test)\n",
    "\n",
    "```\n",
    "\n",
    "## getter e setter\n",
    "\n",
    "- getter - como acessar um atributo privado\n",
    "- setter - como mudar um atributo privado\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class MachineLearningModel:\n",
    "    def __init__(self):\n",
    "        self.__model = LogisticRegression()\n",
    "        self.__hyperparameters = {'C': 1.0}\n",
    "        self.__trained = False\n",
    "\n",
    "    @property\n",
    "    def hyperparameters(self):\n",
    "        return self.__hyperparameters\n",
    "    \n",
    "    @hyperparameters.setter\n",
    "    def hyperparameters(self, params):\n",
    "        if isinstance(params, dict): # isso aqui é uma validação para ver se foi passado um dicionario, caso contrário quebra\n",
    "            self.__hyperparameters.update(params)\n",
    "            self.__model.set_params(**self.__hyperparameters)\n",
    "        else:\n",
    "            raise ValueError(\"Hyperparameters must be a dictionary\")\n",
    "\n",
    "    @property\n",
    "    def trained(self):\n",
    "        return self.__trained\n",
    "\n",
    "    def train(self, X, y):\n",
    "        self.__model.fit(X, y)\n",
    "        self.__trained = True\n",
    "\n",
    "    def predict(self, X):\n",
    "        if not self.__trained:\n",
    "            raise ValueError('O modelo precisa ser treinado antes de fazer previsões')\n",
    "        return self.__model.predict(X)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    X_train = np.array([[1,2],[2,3],[3,4],[4,5],[5,6]])\n",
    "    y_train = np.array([0,0,1,1,1])\n",
    "    \n",
    "    X_test = np.array([[1.5,2.5], [2.5,3,5], [3.5, 4.5]])\n",
    "    \n",
    "    model = MachineLearningModel()\n",
    "    print(\"hiperparametros do modelo:\", model.hyperparameters)\n",
    "    model.hyperparameters = {'C': 0.5}\n",
    "\n",
    "    model.train(X_train, y_train)\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    print(\"previsoes\", predictions)\n",
    "    print(\"modelo treinado: \", model.trained) # acessamos trained através da @property e não diretamente\n",
    "    print(\"hyperparametros: \", model.hyperparameters)\n",
    "```\n",
    "\n",
    "## Manipulação e exceções\n",
    "\n",
    "- ao escrever um algoritmo temos que pensar tanto no resultado que queremos ter quanto no usuário fazendo coisa errada, o que podemos fazer para previnir certos comportamentos, tratar exceções (try except)\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "# a classe abaixo permite customizar mensagens de erro, aí passa DataValidationError após os raise\n",
    "class DataValidationError(Exception):\n",
    "    def __init__(self, message):\n",
    "        self.message = message\n",
    "        super().__init__(self.message)\n",
    "\n",
    "\n",
    "class MachineLearningModel:\n",
    "    def __init__(self):\n",
    "        self.model = LogisticRegression()\n",
    "        self.istrained = False\n",
    "\n",
    "    def train(self, X, y):\n",
    "        try:\n",
    "            if X.size == 0 or y.size == 0:\n",
    "                raise ValueError(\"Os dados não podem estar vazios\")\n",
    "            self.model.fit(X, y)\n",
    "            self.is_trained = True\n",
    "            print(\"modelo treinado com sucesso\") # em produção não faria print, mandaria para um log\n",
    "        except ValueError as e:\n",
    "            print(f\"Erro de valor: {e}\") # em produção não faria print, mandaria para um log\n",
    "        except Exception as e:\n",
    "            print(f\"Ocorreu um erro durante o treinamento: {e}\") # em produção não faria print, mandaria para um log\n",
    "    \n",
    "    def predict(self, X):\n",
    "        try:\n",
    "            if not self.is_trained:\n",
    "                raise NotFittedError(\"O modelo não foi treinado ainda\")\n",
    "            if X.size == 0:\n",
    "                raise ValueError(\"Os dados de predição não podem estar vazios\")\n",
    "            predictions = self.model.predict(X)\n",
    "            return predictions\n",
    "        except NotFittedError as e:\n",
    "            print(f\"Erro de ajuste: {e}\") \n",
    "        except ValueError as e:\n",
    "            print(f\"Erro de valor: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Ocorreu um erro durante a predição: {e}\")\n",
    "            return None\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        try:\n",
    "            predictions = self.predict(X)\n",
    "            if predictions is None:\n",
    "                raise ValueError(\"A predição falhou, não é possível avaliar\")\n",
    "            accuracy = np.mean(predictions == y)\n",
    "            return accuracy\n",
    "        except ValueError as e:\n",
    "            print(f\"Erro de valor: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Ocorreu um erro durante a avaliação: {e}\")\n",
    "            return None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    X_train = np.array([[1,2],[2,3],[3,4],[4,5],[5,6]])\n",
    "    y_train = np.array([0,0,1,1,1])\n",
    "    \n",
    "    X_test = np.array([[1.5,2.5], [2.5,3,5], [3.5, 4.5]])\n",
    "    y_test = np.arra([0,0,1])\n",
    "    \n",
    "    model = MachineLearningModel()\n",
    "    \n",
    "    # simulando erro 1\n",
    "    X_train = np.array([])\n",
    "    \n",
    "    model.train(X_train, y_train)\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "    if predictions is not None:\n",
    "        print(\"previsoes\", predictions)\n",
    "    \n",
    "    accuracy = model.evaluate(X_test, y_test)\n",
    "    if accuracy is not None:\n",
    "        print(\"acuracia\", accuracy)\n",
    "```\n",
    "\n",
    "## doc strings\n",
    "\n",
    "como documentar o código, usar docstring abaixo das classes e funções\n",
    "\n",
    "```python\n",
    "    class MachineLearningModel:\n",
    "        \"\"\"\n",
    "        Classe para gerenciar um pipeline de ML\n",
    "        Inclui prepross, treinamento e avaliação\n",
    "        \n",
    "        Atributos:\n",
    "        model: LogisticRegression\n",
    "            o modelo usado no pipeline\n",
    "        scaler: StandardScaler\n",
    "            o escalador usado para normalizar\n",
    "        \"\"\"\n",
    "\n",
    "    def preprocess(self, X):\n",
    "        \"\"\"\n",
    "        Pre processa os dados, normalizando-os\n",
    "\n",
    "        parametros:\n",
    "        X : np.ndarray\n",
    "            dados de entrada para normalizar\n",
    "        retorna:\n",
    "        np.ndarray\n",
    "            dados normalizados\n",
    "        \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Fast API\n",
    "\n",
    "## Aula 1\n",
    "\n",
    "Fast API e flask são duas ferramentas:\n",
    "- FAST API é ASGI (assíncrono) - não ocorre ao mesmo tempo, é o futuro da programação - fazer as coisas em paralelo. Vantagem é escala! alinha bem com coisas de ML\n",
    "- Flask é WSGI (web server gateway interface) - mais alinhado com as páginas dinamicas de hoje em dia - página da amazon gerada dinamicamente com base na pesquisa (nao foi criada previamente, foi gerada dinamicamente)\n",
    "\n",
    "Websocket - permite canais de comunicação client-server - caminho fica aberto de duas vias, ajuda em ML e chats\n",
    "\n",
    "servidor uvicorn - assíncrono, aceita varias requisições e faz em paralelo\n",
    "\n",
    "## Aula 2 - Joy Model com FastAPI\n",
    "\n",
    "Ver arquivos:\n",
    "- train_api.py\n",
    "- train.py\n",
    "\n",
    "## Aula 3 \n",
    "\n",
    "Rota x Endpoint\n",
    "- Rota: o que vem depois da / na URL, e está associada a um método (função) no backend. Deve expressar o que ela faz, a funcionalidade dela\n",
    "- Endpoint: Rota + método (verbo) - GET, POST, PUT, DELETE. O endpoint incluir o verbo HTTP\n",
    "Posso ter a mesma rota com dois endpoints diferentes, basta ter os verbos diferentes\n",
    "\n",
    "## Aula 4 Pydantic\n",
    "\n",
    "Usada para validação de dados. Representar os dados através de uma classe que herda o BaseModel. Podemos também implementar validação de dados.\n",
    "\n",
    "Podemos validar tanto o que recebemos quanto o que enviamos.\n",
    "\n",
    "## Aula 5 - Testes automatizados\n",
    "\n",
    "Testes para garantir a qualidade\n",
    "\n",
    "Biblioteca pytest\n",
    "\n",
    "Testar endpoints, classes, funções, etc\n",
    "\n",
    "criar testes para as funções dos endpoints, códigos e mensagens de erro, etc\n",
    "\n",
    "ver arquivo test.py e train_api_v2.py\n",
    "\n",
    "## Aula 6 - documentação\n",
    "\n",
    "Fast API já tem ferramenta de documentação e ambiente de teste, basta subir a API no servidor local, abrir no browser e digitar /docs depois da url ou /redoc\n",
    "\n",
    "ver train_api_v3.py"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
